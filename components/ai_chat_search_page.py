import streamlit as st
import google.generativeai as genai

def render_ai_chat_search():
    st.header("ğŸ¤– AI æˆ¿å¸‚é¡§å•")
    st.write("ä½ å¯ä»¥è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢æ¢ä»¶ï¼ŒAI æœƒå¹«ä½ æœå°‹é©åˆçš„ç‰©ä»¶ã€‚")
    
    # ====== GEMINI_KEY é©—è­‰ ======
    gemini_key = st.session_state.get("GEMINI_KEY", "")
    if not gemini_key:
        st.error("âŒ å³å´ gemini API Key æœªè¨­å®šæˆ–æœ‰èª¤")
        st.stop()
    
    # ====== åˆå§‹åŒ– Gemini API ======
    try:
        genai.configure(api_key=gemini_key)
    except Exception as e:
        st.error(f"âŒ Gemini åˆå§‹åŒ–éŒ¯èª¤ï¼š{e}")
        st.stop()
    
    # ====== èŠå¤©è¨˜éŒ„ ======
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # ====== é¡¯ç¤ºèŠå¤©è¨˜éŒ„ ======
    for chat in st.session_state.chat_history:
        with st.chat_message(chat["role"]):
            st.markdown(chat["content"])
    
    # ====== ä½¿ç”¨è€…è¼¸å…¥ ======
    if prompt := st.chat_input("è«‹è¼¸å…¥æŸ¥è©¢æ¢ä»¶ï¼Œä¾‹å¦‚ï¼šã€å°åŒ— 2000 è¬å…§ 3 æˆ¿ã€"):
        # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
        st.chat_message("user").markdown(prompt)
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        try:
            # å‘¼å« Gemini AI
            resp = genai.chat(
                model="gemini-2.0-flash",
                messages=[{"role": "user", "content": prompt}]
            )
            ai_reply = resp.last or resp.response  # ä¾ SDK ç‰ˆæœ¬
        except Exception as e:
            ai_reply = f"âŒ API ç™¼ç”ŸéŒ¯èª¤: {e}"
        
        # é¡¯ç¤º AI è¨Šæ¯
        st.chat_message("assistant").markdown(ai_reply)
        st.session_state.chat_history.append({"role": "ai", "content": ai_reply})
